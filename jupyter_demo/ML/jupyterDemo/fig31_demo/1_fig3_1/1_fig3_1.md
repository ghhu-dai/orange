# 机器学习的一般步骤
## 1. 数据收集
## 2. 数据预处理
### <font color="blue">包括数据清洗，数据标准化，特征工程等</font>
## 3. 模型构建
## 4. 模型训练
## 5. 模型预测
## 6. 模型评估
### <font color="blue">常用的评估指标有：均方根误差，平均绝对误差等</font>

## 7. 模型调优
###  <font color="blue">根据模型评估的结果，对模型进行评估，包括调整超参数，改进特征工程，调整核函数以提高模型性能</font>

 

# 对电池35C02的容量预测



 

<!-- ###  导入机器学习的相关工具 -->



```python
# from IPython.display import display_html
# display_html("""<button onclick="$('.input,  .output_stderr, .output_error').toggle();">Toggle Code</button>""", raw=True)


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF,ConstantKernel
from sklearn.gaussian_process.kernels import WhiteKernel
from sklearn.preprocessing import StandardScaler,MinMaxScaler
from sklearn.model_selection import GridSearchCV
```

### 数据集收集与处理

 直接使用论文收集好的训练集和测试集，并且无需再进行特征抽取


```python

filenames = ['EIS_data.txt', 'Capacity_data.txt', 'EIS_data_35C02.txt', 'capacity35C02.txt']
EIS_data = np.loadtxt(filenames[0],delimiter='\t')
Capacity_data = np.loadtxt(filenames[1],delimiter='\t')
EIS_data_35C02 = np.loadtxt(filenames[2],delimiter='\t')
capacity35C02 = np.loadtxt(filenames[3],delimiter='\t')


EIS_data_new = pd.DataFrame(EIS_data)
Capacity_data_new = pd.DataFrame(Capacity_data)
columns=[]
for i in range(1,61):
        columns.append('Real Z'+str(i))
    
for i in range(1,61):
        columns.append('Imaginary Z'+str(i))   
    

    
EIS_data_new.columns=columns
Capacity_data_new.columns = ['Capacity']


```

### 输入数据：电化学阻抗谱一个循环周期内在60个不同频率点上的实部和虚部

一共1358个样本，每个样本有120个特征


```python

EIS_data_new = pd.DataFrame(EIS_data)
Capacity_data_new = pd.DataFrame(Capacity_data)
columns=[]
for i in range(1,61):
        columns.append('Real Z'+str(i))
    
for i in range(1,61):
        columns.append('Imaginary Z'+str(i))   
    

    
EIS_data_new.columns=columns
Capacity_data_new.columns = ['Capacity']
EIS_data_new # 训练集_输入（电化学阻抗谱）
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Real Z1</th>
      <th>Real Z2</th>
      <th>Real Z3</th>
      <th>Real Z4</th>
      <th>Real Z5</th>
      <th>Real Z6</th>
      <th>Real Z7</th>
      <th>Real Z8</th>
      <th>Real Z9</th>
      <th>Real Z10</th>
      <th>...</th>
      <th>Imaginary Z51</th>
      <th>Imaginary Z52</th>
      <th>Imaginary Z53</th>
      <th>Imaginary Z54</th>
      <th>Imaginary Z55</th>
      <th>Imaginary Z56</th>
      <th>Imaginary Z57</th>
      <th>Imaginary Z58</th>
      <th>Imaginary Z59</th>
      <th>Imaginary Z60</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.38470</td>
      <td>0.39156</td>
      <td>0.39684</td>
      <td>0.40341</td>
      <td>0.40963</td>
      <td>0.41925</td>
      <td>0.42764</td>
      <td>0.43530</td>
      <td>0.44525</td>
      <td>0.45744</td>
      <td>...</td>
      <td>0.08888</td>
      <td>0.10230</td>
      <td>0.11767</td>
      <td>0.13411</td>
      <td>0.15473</td>
      <td>0.18024</td>
      <td>0.21535</td>
      <td>0.25411</td>
      <td>0.29026</td>
      <td>0.32795</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.38886</td>
      <td>0.39343</td>
      <td>0.40066</td>
      <td>0.40798</td>
      <td>0.41472</td>
      <td>0.42210</td>
      <td>0.43087</td>
      <td>0.44032</td>
      <td>0.45061</td>
      <td>0.46045</td>
      <td>...</td>
      <td>0.09005</td>
      <td>0.10526</td>
      <td>0.12138</td>
      <td>0.13894</td>
      <td>0.15901</td>
      <td>0.18200</td>
      <td>0.21310</td>
      <td>0.24700</td>
      <td>0.28064</td>
      <td>0.32300</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.39038</td>
      <td>0.39600</td>
      <td>0.40250</td>
      <td>0.40874</td>
      <td>0.41617</td>
      <td>0.42422</td>
      <td>0.43090</td>
      <td>0.44257</td>
      <td>0.45141</td>
      <td>0.46294</td>
      <td>...</td>
      <td>0.09073</td>
      <td>0.10267</td>
      <td>0.11865</td>
      <td>0.14024</td>
      <td>0.16487</td>
      <td>0.18764</td>
      <td>0.21288</td>
      <td>0.24707</td>
      <td>0.28773</td>
      <td>0.32955</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.39194</td>
      <td>0.39643</td>
      <td>0.40406</td>
      <td>0.41059</td>
      <td>0.41739</td>
      <td>0.42510</td>
      <td>0.43446</td>
      <td>0.44279</td>
      <td>0.45283</td>
      <td>0.46578</td>
      <td>...</td>
      <td>0.09112</td>
      <td>0.10434</td>
      <td>0.12045</td>
      <td>0.13810</td>
      <td>0.15939</td>
      <td>0.18213</td>
      <td>0.21587</td>
      <td>0.25741</td>
      <td>0.29768</td>
      <td>0.33673</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.39225</td>
      <td>0.39760</td>
      <td>0.40540</td>
      <td>0.41088</td>
      <td>0.41967</td>
      <td>0.42648</td>
      <td>0.43666</td>
      <td>0.44412</td>
      <td>0.45343</td>
      <td>0.46610</td>
      <td>...</td>
      <td>0.09144</td>
      <td>0.10645</td>
      <td>0.12356</td>
      <td>0.14088</td>
      <td>0.16121</td>
      <td>0.18483</td>
      <td>0.21609</td>
      <td>0.24984</td>
      <td>0.28396</td>
      <td>0.32562</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1353</th>
      <td>0.77228</td>
      <td>0.77992</td>
      <td>0.78750</td>
      <td>0.79594</td>
      <td>0.80225</td>
      <td>0.80833</td>
      <td>0.81657</td>
      <td>0.82278</td>
      <td>0.83013</td>
      <td>0.83697</td>
      <td>...</td>
      <td>0.08681</td>
      <td>0.10118</td>
      <td>0.11621</td>
      <td>0.13356</td>
      <td>0.15785</td>
      <td>0.18904</td>
      <td>0.22356</td>
      <td>0.25456</td>
      <td>0.28406</td>
      <td>0.31743</td>
    </tr>
    <tr>
      <th>1354</th>
      <td>0.77439</td>
      <td>0.78211</td>
      <td>0.78999</td>
      <td>0.79706</td>
      <td>0.80398</td>
      <td>0.81029</td>
      <td>0.81627</td>
      <td>0.82467</td>
      <td>0.83234</td>
      <td>0.83909</td>
      <td>...</td>
      <td>0.08563</td>
      <td>0.09931</td>
      <td>0.11901</td>
      <td>0.14191</td>
      <td>0.16804</td>
      <td>0.18878</td>
      <td>0.21408</td>
      <td>0.24479</td>
      <td>0.27689</td>
      <td>0.30725</td>
    </tr>
    <tr>
      <th>1355</th>
      <td>0.77434</td>
      <td>0.78063</td>
      <td>0.78968</td>
      <td>0.79724</td>
      <td>0.80501</td>
      <td>0.81171</td>
      <td>0.81882</td>
      <td>0.82585</td>
      <td>0.83247</td>
      <td>0.83993</td>
      <td>...</td>
      <td>0.08606</td>
      <td>0.09905</td>
      <td>0.11752</td>
      <td>0.14136</td>
      <td>0.16816</td>
      <td>0.19209</td>
      <td>0.21542</td>
      <td>0.24478</td>
      <td>0.27801</td>
      <td>0.30984</td>
    </tr>
    <tr>
      <th>1356</th>
      <td>0.77357</td>
      <td>0.78210</td>
      <td>0.79025</td>
      <td>0.79816</td>
      <td>0.80343</td>
      <td>0.81125</td>
      <td>0.81834</td>
      <td>0.82644</td>
      <td>0.83290</td>
      <td>0.84006</td>
      <td>...</td>
      <td>0.08681</td>
      <td>0.09961</td>
      <td>0.11543</td>
      <td>0.13883</td>
      <td>0.16643</td>
      <td>0.19362</td>
      <td>0.21829</td>
      <td>0.24592</td>
      <td>0.27964</td>
      <td>0.31379</td>
    </tr>
    <tr>
      <th>1357</th>
      <td>0.77241</td>
      <td>0.78241</td>
      <td>0.78999</td>
      <td>0.79711</td>
      <td>0.80445</td>
      <td>0.81036</td>
      <td>0.82028</td>
      <td>0.82552</td>
      <td>0.83178</td>
      <td>0.84059</td>
      <td>...</td>
      <td>0.08683</td>
      <td>0.09916</td>
      <td>0.11552</td>
      <td>0.13887</td>
      <td>0.16656</td>
      <td>0.19308</td>
      <td>0.21782</td>
      <td>0.24541</td>
      <td>0.27908</td>
      <td>0.31301</td>
    </tr>
  </tbody>
</table>
<p>1358 rows × 120 columns</p>
</div>



### 输出数据：电池的容量值


```python
Capacity_data_new # 训练集_输出
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Capacity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>37.20271</td>
    </tr>
    <tr>
      <th>1</th>
      <td>36.22303</td>
    </tr>
    <tr>
      <th>2</th>
      <td>35.58930</td>
    </tr>
    <tr>
      <th>3</th>
      <td>35.10808</td>
    </tr>
    <tr>
      <th>4</th>
      <td>34.76728</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
    </tr>
    <tr>
      <th>1353</th>
      <td>31.05208</td>
    </tr>
    <tr>
      <th>1354</th>
      <td>30.99726</td>
    </tr>
    <tr>
      <th>1355</th>
      <td>31.01811</td>
    </tr>
    <tr>
      <th>1356</th>
      <td>30.98132</td>
    </tr>
    <tr>
      <th>1357</th>
      <td>30.92150</td>
    </tr>
  </tbody>
</table>
<p>1358 rows × 1 columns</p>
</div>



### 对训练集进行标准化
解决数据指标之间的可比性，使各指标处于同一数量级，适合进行综合对比评价


```python

scaler = StandardScaler()

# X_train是标准化之后的模型输入
X_train = scaler.fit_transform(EIS_data)

# Y_train是模型输出
Y_train = Capacity_data

X_train_new = pd.DataFrame(X_train)
X_train_new
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>110</th>
      <th>111</th>
      <th>112</th>
      <th>113</th>
      <th>114</th>
      <th>115</th>
      <th>116</th>
      <th>117</th>
      <th>118</th>
      <th>119</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-0.147749</td>
      <td>-0.147580</td>
      <td>-0.155488</td>
      <td>-0.160018</td>
      <td>-0.167590</td>
      <td>-0.163949</td>
      <td>-0.167770</td>
      <td>-0.176121</td>
      <td>-0.177981</td>
      <td>-0.173665</td>
      <td>...</td>
      <td>-0.743492</td>
      <td>-0.752219</td>
      <td>-0.791432</td>
      <td>-0.898690</td>
      <td>-0.958177</td>
      <td>-0.867309</td>
      <td>-0.619771</td>
      <td>-0.441471</td>
      <td>-0.442363</td>
      <td>-0.497866</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.132036</td>
      <td>-0.140508</td>
      <td>-0.141010</td>
      <td>-0.142648</td>
      <td>-0.148175</td>
      <td>-0.153033</td>
      <td>-0.155335</td>
      <td>-0.156681</td>
      <td>-0.157086</td>
      <td>-0.161844</td>
      <td>...</td>
      <td>-0.692410</td>
      <td>-0.618570</td>
      <td>-0.624116</td>
      <td>-0.689998</td>
      <td>-0.788379</td>
      <td>-0.804651</td>
      <td>-0.690144</td>
      <td>-0.632713</td>
      <td>-0.662606</td>
      <td>-0.594180</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.126295</td>
      <td>-0.130789</td>
      <td>-0.134036</td>
      <td>-0.139760</td>
      <td>-0.142644</td>
      <td>-0.144912</td>
      <td>-0.155220</td>
      <td>-0.147968</td>
      <td>-0.153968</td>
      <td>-0.152065</td>
      <td>...</td>
      <td>-0.662722</td>
      <td>-0.735513</td>
      <td>-0.747236</td>
      <td>-0.633828</td>
      <td>-0.555899</td>
      <td>-0.603862</td>
      <td>-0.697025</td>
      <td>-0.630830</td>
      <td>-0.500285</td>
      <td>-0.466734</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.120402</td>
      <td>-0.129163</td>
      <td>-0.128124</td>
      <td>-0.132728</td>
      <td>-0.137991</td>
      <td>-0.141542</td>
      <td>-0.141514</td>
      <td>-0.147116</td>
      <td>-0.148432</td>
      <td>-0.140912</td>
      <td>...</td>
      <td>-0.645695</td>
      <td>-0.660109</td>
      <td>-0.666058</td>
      <td>-0.726292</td>
      <td>-0.773303</td>
      <td>-0.800023</td>
      <td>-0.603507</td>
      <td>-0.352709</td>
      <td>-0.272487</td>
      <td>-0.327030</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-0.119231</td>
      <td>-0.124738</td>
      <td>-0.123046</td>
      <td>-0.131626</td>
      <td>-0.129294</td>
      <td>-0.136256</td>
      <td>-0.133044</td>
      <td>-0.141965</td>
      <td>-0.146093</td>
      <td>-0.139655</td>
      <td>...</td>
      <td>-0.631724</td>
      <td>-0.564839</td>
      <td>-0.525801</td>
      <td>-0.606176</td>
      <td>-0.701100</td>
      <td>-0.703901</td>
      <td>-0.596626</td>
      <td>-0.556324</td>
      <td>-0.586597</td>
      <td>-0.543202</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1353</th>
      <td>1.316216</td>
      <td>1.321098</td>
      <td>1.325105</td>
      <td>1.331885</td>
      <td>1.329976</td>
      <td>1.326350</td>
      <td>1.329581</td>
      <td>1.324401</td>
      <td>1.322374</td>
      <td>1.316853</td>
      <td>...</td>
      <td>-0.833866</td>
      <td>-0.802790</td>
      <td>-0.857277</td>
      <td>-0.922454</td>
      <td>-0.834399</td>
      <td>-0.554020</td>
      <td>-0.362989</td>
      <td>-0.429367</td>
      <td>-0.584308</td>
      <td>-0.702558</td>
    </tr>
    <tr>
      <th>1354</th>
      <td>1.324186</td>
      <td>1.329380</td>
      <td>1.334542</td>
      <td>1.336142</td>
      <td>1.336574</td>
      <td>1.333857</td>
      <td>1.328426</td>
      <td>1.331720</td>
      <td>1.330989</td>
      <td>1.325179</td>
      <td>...</td>
      <td>-0.885384</td>
      <td>-0.887224</td>
      <td>-0.731000</td>
      <td>-0.561672</td>
      <td>-0.430137</td>
      <td>-0.563276</td>
      <td>-0.659492</td>
      <td>-0.692157</td>
      <td>-0.748460</td>
      <td>-0.900634</td>
    </tr>
    <tr>
      <th>1355</th>
      <td>1.323997</td>
      <td>1.323783</td>
      <td>1.333367</td>
      <td>1.336826</td>
      <td>1.340503</td>
      <td>1.339297</td>
      <td>1.338243</td>
      <td>1.336290</td>
      <td>1.331496</td>
      <td>1.328478</td>
      <td>...</td>
      <td>-0.866610</td>
      <td>-0.898963</td>
      <td>-0.798197</td>
      <td>-0.585436</td>
      <td>-0.425376</td>
      <td>-0.445437</td>
      <td>-0.617582</td>
      <td>-0.692426</td>
      <td>-0.722819</td>
      <td>-0.850240</td>
    </tr>
    <tr>
      <th>1356</th>
      <td>1.321088</td>
      <td>1.329343</td>
      <td>1.335527</td>
      <td>1.340323</td>
      <td>1.334476</td>
      <td>1.337535</td>
      <td>1.336395</td>
      <td>1.338574</td>
      <td>1.333172</td>
      <td>1.328988</td>
      <td>...</td>
      <td>-0.833866</td>
      <td>-0.873678</td>
      <td>-0.892454</td>
      <td>-0.694751</td>
      <td>-0.494010</td>
      <td>-0.390967</td>
      <td>-0.527817</td>
      <td>-0.661763</td>
      <td>-0.685501</td>
      <td>-0.773383</td>
    </tr>
    <tr>
      <th>1357</th>
      <td>1.316707</td>
      <td>1.330515</td>
      <td>1.334542</td>
      <td>1.336332</td>
      <td>1.338367</td>
      <td>1.334126</td>
      <td>1.343864</td>
      <td>1.335012</td>
      <td>1.328806</td>
      <td>1.331070</td>
      <td>...</td>
      <td>-0.832993</td>
      <td>-0.893996</td>
      <td>-0.888395</td>
      <td>-0.693023</td>
      <td>-0.488852</td>
      <td>-0.410192</td>
      <td>-0.542517</td>
      <td>-0.675480</td>
      <td>-0.698322</td>
      <td>-0.788560</td>
    </tr>
  </tbody>
</table>
<p>1358 rows × 120 columns</p>
</div>



 


```python

```

### 构建高斯过程回归模型并训练模型


```python

from IPython.display import display_html
display_html("""<button onclick="$('.input,  .output_stderr, .output_error').toggle();">Toggle Code</button>""", raw=True)

# param_grid = {'alpha': np.logspace(-3, 3, 7),
#               'kernel': [1.0*RBF(length_scale=1.0),
#                          2.0*RBF(length_scale=1.0),
#                          5.0*RBF(length_scale=1.0)]}

# grid_search = GridSearchCV(gpr, param_grid=param_grid, cv=5)

# grid_search.fit(X_train, Y_train)

# # 输出最优超参数
# print('Best parameters:', grid_search.best_params_)

# # 输出最优模型的R2得分
# print('Best R2 score:', grid_search.best_score_)



#模型构建
gpr = GaussianProcessRegressor(alpha=1,kernel=5*RBF(),n_restarts_optimizer=5)
'''
    alpha : 正则化参数，控制模型的复杂度，避免过拟合（指模型在训练数据上表现很好，在测试数据上表现不佳）
    kernel: 协方差函数，用于计算样本之间的相似度，这里用的是径向基函数（高斯核函数）利于处理高维空间的数据
    n_restarts_optimizer: 优化器的重启次数，用于寻找最佳的超参数值。
    
'''
# 模型训练
gpr.fit(X_train,Y_train)
```


<button onclick="$('.input,  .output_stderr, .output_error').toggle();">Toggle Code</button>





<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-9" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>GaussianProcessRegressor(alpha=1, kernel=2.24**2 * RBF(length_scale=1),
                         n_restarts_optimizer=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-9" type="checkbox" checked><label for="sk-estimator-id-9" class="sk-toggleable__label sk-toggleable__label-arrow">GaussianProcessRegressor</label><div class="sk-toggleable__content"><pre>GaussianProcessRegressor(alpha=1, kernel=2.24**2 * RBF(length_scale=1),
                         n_restarts_optimizer=5)</pre></div></div></div></div></div>



### 模型评估

 评估参数使用决定系数 R^2，用于评估模型对观察到的数据拟合的好坏程度，范围在0和1之间。值越接近1，表示模型对数据的拟合效果越好。

 $R^2 = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}$

其中，$y_i$ 表示观测到的实际值，$\hat{y_i}$ 表示模型的预测值，$\bar{y}$ 表示所有实际值的平均数。


```python
# Testing set of the GPR model
# X_test = StandardScaler().fit_transform(EIS_data_35C02)
X_test = (EIS_data_35C02-mean)/std
Y_test = capacity35C02
# Y_test_35C02 = StandardScaler().fit_transform(capacity35C02)
# Capacity estimation of the testing cell
Y_pred, Y_pred_var = gpr.predict(X_test, return_std=True)


r2 = gpr.score(X_test, Y_test) 
print('r^2 = ',r2)
```

    r^2 =  0.7347521656523712


### 模型预测值和实际测量值的数据对比


```python
# y_pred=scale_y.inverse_transform(y_pred)
pd.DataFrame([Y_test,Y_pred])
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>289</th>
      <th>290</th>
      <th>291</th>
      <th>292</th>
      <th>293</th>
      <th>294</th>
      <th>295</th>
      <th>296</th>
      <th>297</th>
      <th>298</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>40.473770</td>
      <td>39.741610</td>
      <td>39.173800</td>
      <td>38.65528</td>
      <td>38.226070</td>
      <td>37.87145</td>
      <td>37.592520</td>
      <td>37.361910</td>
      <td>37.199870</td>
      <td>37.190530</td>
      <td>...</td>
      <td>27.501600</td>
      <td>27.742930</td>
      <td>27.833410</td>
      <td>27.827820</td>
      <td>27.741430</td>
      <td>27.570170</td>
      <td>27.525850</td>
      <td>27.558680</td>
      <td>27.596250</td>
      <td>27.543000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>39.111511</td>
      <td>38.694237</td>
      <td>38.484359</td>
      <td>38.30799</td>
      <td>38.094839</td>
      <td>37.92869</td>
      <td>37.797767</td>
      <td>37.695881</td>
      <td>37.674306</td>
      <td>37.709745</td>
      <td>...</td>
      <td>29.667003</td>
      <td>29.815167</td>
      <td>29.823219</td>
      <td>29.838322</td>
      <td>29.725555</td>
      <td>29.590438</td>
      <td>29.569919</td>
      <td>29.729428</td>
      <td>29.633352</td>
      <td>29.634588</td>
    </tr>
  </tbody>
</table>
<p>2 rows × 299 columns</p>
</div>




```python

# plot
print('r^2 = ',r2)

Y_pred_norm = Y_pred / Y_pred[0] + Y_pred_var / Y_pred[0]
Y_pred_norm_lower = Y_pred / Y_pred[0] - Y_pred_var / Y_pred[0]
fig = plt.figure(figsize=(10, 5))
# plt.fill_between(np.arange(2, 600, 2), Y_pred_norm_lower, Y_pred_norm, color='mistyrose')
plt.plot(np.arange(2, 600, 2),capacity35C02/capacity35C02[0])
plt.plot(np.arange(2, 600, 2),Y_pred/Y_pred[0])

plt.xlim([0, 400])
plt.ylim([0.7, 1.045])


plt.xlabel('Cycle Number', fontsize=25)
plt.ylabel('Capacity', fontsize=25)
plt.title('35C02', fontsize=25)
plt.legend(['Measured', 'Estimated'], loc='upper right', fontsize=12)
plt.show()

```

    r^2 =  0.7347521656523712




![png](output_22_1.png)
    


![](./fig31.png)

### 模型表现不佳的可能原因

通过与论文提供的源码与数据比对，模型的数据集与特征处理，以及`kernel函数`的类型选择没有问题
问题可能出在对于模型其他超参数的优化方法上有所区别，论文里提供的方法是`最大化边缘似然函数`

`边缘似然函数`是模型参数的积分，可以看作是模型在所有可能的参数下能够生成观测数据的概率，即该数据集在参数空间上的分布，边缘似然函数的优化目的就是要找到最优的超参数值使得边缘似然函数最大化

后续会尝试使用`最大化边缘似然函数`来优化超参数


```python

```


```python

```
